{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#taking only upto 3 decimal places\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "#reading the audio database into a dataframe\n",
        "audio_data = pd.read_csv(\"/content/sample_data/Spectral_Feature.csv\")\n",
        "\n",
        "#taking only the rows with values and dropping nan and empty rows\n",
        "nan_value = float(\"NaN\")\n",
        "audio_data = audio_data[audio_data['gender'].notna()]\n",
        "#dividing data into test and train\n",
        "audio_data_train, audio_data_test = train_test_split(audio_data, test_size=0.2, random_state=25)\n",
        "#extracting input and output training dataset (features) from excel\n",
        "audio_data_train_features = np.array(audio_data_train.iloc[: , 1: 8])\n",
        "audio_data_train_labels = np.array(audio_data_train.iloc[: , 8: 9])\n",
        "\n",
        "#extracting input and output testing dataset (features) from excel\n",
        "audio_data_test_features = np.array(audio_data_test.iloc[: ,1 : 8])\n",
        "audio_data_test_labels = np.array(audio_data_test.iloc[: , 8: 9])\n",
        "\n",
        "#checking array shape \n",
        "print(f\"No. of training examples: {audio_data_train_features.shape[0]}\")\n",
        "print(f\"No. of testing examples: {audio_data_train_labels.shape[0]}\")\n",
        "print(f\"No. of training examples: {audio_data_test_features.shape[0]}\")\n",
        "print(f\"No. of testing examples: {audio_data_test_labels.shape[0]}\")\n",
        "\n",
        "#checking the arrays to be fed\n",
        "print(audio_data_train_features[0:1])\n",
        "print(audio_data_train_labels[0:1])\n",
        "print(audio_data_test_features[0:1])\n",
        "print(audio_data_test_labels[0:1])\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation = 'softmax'),\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(audio_data_train_features, audio_data_train_labels, epochs = 1)\n",
        "\n",
        "\n",
        "#add correlation matrix===fscore\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BrMvzq839b8",
        "outputId": "d781837a-92cc-4d28-b8f4-96aecbbba24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of training examples: 12979\n",
            "No. of testing examples: 12979\n",
            "No. of training examples: 3245\n",
            "No. of testing examples: 3245\n",
            "[[   0.608    0.    2459.02  2863.411 6065.101    0.151    0.   ]]\n",
            "[[0.]]\n",
            "[[   0.389    0.03  2118.247 1856.83  3971.604    0.146    0.   ]]\n",
            "[[0.]]\n",
            "406/406 [==============================] - 5s 3ms/step - loss: 0.9023 - accuracy: 0.7198\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd5b037e9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(audio_data_test_features, audio_data_test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bghiILlitA4t",
        "outputId": "f46a66f3-dee7-4079-911b-ce2eb42d5950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102/102 [==============================] - 0s 2ms/step - loss: 0.6777 - accuracy: 0.7735\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6777464151382446, 0.7734977006912231]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = np.array([0.522, 0.066, 796.363, 1390.839, 0.038])\n",
        "print(model.predict(test.reshape(1.,8), batch_size = 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "qYjQT5IFfo4W",
        "outputId": "2562f1f1-069f-4b33-90d8-d855fce8df43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d3f5d0e2a278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.522\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.066\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m796.363\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1390.839\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.038\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VpCYsCq3kN4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}